{"type": "root", "attrs": {"type": "node", "context": "top", "internal": "mlregressiontrain", "icon": "ROP/mlregressiontrain", "since": "20.5", "version": null, "namespace": null}, "body": [{"level": 0, "type": "title", "indent": 0, "text": ["ML Regression Train"], "extent": [107, 131]}, {"type": "summary", "indent": 0, "text": ["Trains a neural network for regression."], "extent": [131, 179]}, {"level": 2, "id": "oview", "container": true, "type": "h", "indent": 0, "text": ["Overview"], "extent": [179, 202], "body": [{"type": "para", "indent": 0, "text": ["This is a generic ML training node that can solve regression problems. In ML, regression means training a feedforward neural network (a model) so it can closely approximate a continuous function from a fixed number of input variables to a fixed number of output variables. In Houdini, this function may be provided in the form of a procedural network.\nFor example, regression ML may learn how to realistically deform a character based on its rig pose, improving over linear blend skinning, as shown by the ML Deformer content library scene. In this case, you use ML to learn the function that maps a rig pose to a skin deformation."], "extent": [202, 836]}, {"type": "para", "indent": 0, "text": ["ML Regression Train trains a model given a set of data set consisting of labeled examples, see ", {"scheme": "Node", "value": "/nodes/sop/mlexample", "type": "link", "text": ["ML Example"], "fullpath": "/nodes/sop/mlexample.html"}, ". Each labeled example is a pair consisting of an input value and a corresponding\ntarget value. For regression, both the input value and the output value are tuples of continuous variables, which may include point coordinates, colors, or PCA components. The term ", {"type": "ui", "text": ["regression"]}, " is used because the output consists of continuous variables. Labeled examples are the basis for training. In the specific case of the ML Deformer, an input is an (encoded) pose and the corresponding target is the (encoded) skin deformation obtained by a flesh simulation."], "extent": [836, 1513]}, {"type": "para", "indent": 0, "text": ["ML Regression Train creates a model that predicts a target value given some user-specified input value. The goal is to have a model that is generalized well; it should produce useful predictions for inputs that are not included in the training portion of the data set. ML Regression Train provides several regularization techniques, which are techniques aimed at helping to ensure good generalization to unseen inputs."], "extent": [1513, 1933]}, {"type": "para", "indent": 0, "text": ["The trained model produced by ML Regression Train is written out to disk in ONNX format. The easiest way to bring this model into Houdini is the ", {"scheme": "Node", "value": "/nodes/sop/mlregressioninference", "type": "link", "text": ["ML Regression Inference"], "fullpath": "/nodes/sop/mlregressioninference.html"}, " tool. "], "extent": [1933, 2143]}, {"type": "para", "indent": 0, "text": ["ML Regression Train creates a feedforward neural network. The number of inputs in this network corresponds to the number of input variables. The number of outputs of the network corresponds to the number of target variables. ML Regression Train automatically figures out the number of inputs and outputs of the network using the dimensions stored in the raw data set that it works with."], "extent": [2143, 2531]}, {"type": "para", "indent": 0, "text": ["In-between there are several hidden layers, which can be controlled by the user. The width of these hidden layers is the maximum of the number of inputs and outputs. The behavior of the network is controlled by a set of parameters. Each parameter is either a weight (scaling factor) or a bias (additive constant). The training aims to optimize these parameters for the regression task."], "extent": [2531, 2918]}, {"type": "para", "indent": 0, "text": ["The training process consists of several passes through the training set (the training portion of the data set). Each such pass is called an ", {"type": "em", "text": ["epoch"]}, "."], "extent": [2918, 3069]}, {"type": "para", "indent": 0, "text": ["The training process may be repeated multiple times, each time with different settings on the training node. These training settings are commonly referred to as ", {"type": "em", "text": ["hyperparameters"]}, ".\nThe ", {"scheme": "Node", "value": "/nodes/top/wedge", "type": "link", "text": ["Wedge TOP"], "fullpath": "/nodes/top/wedge.html"}, " allows the training process to be repeated with varying hyperparameters. For each setting in a wedge, the resulting model can be saved to a separate file by adding a TOP attribute name at the end of the ", {"type": "ui", "text": ["Model Base Name"]}, "."], "extent": [3069, 3505]}, {"type": "para", "indent": 0, "text": ["ML Regression Train internally splits the data set provided by the user into two portions: a training portion and a validation portion. These portions can be controlled by the user. The training portion is used to improve the network\u2019s parameters during each training epoch.\nThe validation portion is used to verify the network\u2019s performance on data that is not used for the training."], "extent": [3505, 3891]}, {"type": "para", "indent": 0, "text": ["Overfitting occurs when a model produces outputs that are accurate for inputs in the training data but inaccurate for inputs outside the training data (unseen). To reduce overfitting, the ML Regression Train TOP supports two simple regularization strategies: early stopping and weight decay.\nEarly stopping checks whether the validation loss (the loss on the validation set) stops decreasing. If this is the case, then the training terminates. Weight decay modifies the loss function that is used for training to include a term that is aimed at keeping the weights of the neural network (not the biases) small."], "extent": [3891, 4503]}]}, {"level": 2, "id": null, "container": true, "type": "h", "indent": 0, "text": ["Partial Training"], "extent": [4503, 4526], "body": [{"type": "para", "indent": 0, "text": ["Instead of training a model all at once, a model can be partially trained bit by bit, where each training pass consists of a limited number of epochs. Breaking up the training into several passes offers various training setup possibilities. For example, it allows partial models to be periodically output and retained. This allows you to see how training progresses by visualizing the output of partially trained models. "], "extent": [4526, 4950]}, {"type": "para", "indent": 0, "text": ["Another training setup is to generate fresh training data before each partial training pass. This may be applicable when the data set is generated within Houdini through a procedural network."], "extent": [4950, 5143]}, {"type": "para", "indent": 0, "text": ["Training on a stream of fresh training data compared to revisiting data points in a fixed-size data set may produce a model that generalizes better to unseen data. Another advantage is the amount of training data does not have to be decided upfront."], "extent": [5143, 5394]}, {"type": "para", "indent": 0, "text": ["Breaking up the training into several passes can be achieved by placing and configuring TOP nodes around ML Train Regression as follows:"], "extent": [5394, 5531]}, {"type": "bullet_group", "body": [{"blevel": 2, "type": "bullet", "indent": 0, "text": ["ML Regression Train can be placed inside a feedback block, see ", {"scheme": "Node", "value": "/nodes/top/feedbackbegin", "type": "link", "text": ["Feedback Begin"], "fullpath": "/nodes/top/feedbackbegin.html"}, "."], "extent": [5531, 5637]}, {"blevel": 2, "type": "bullet", "indent": 0, "text": ["The input to ", {"scheme": "Node", "value": "/nodes/top/feedbackbegin", "type": "link", "text": ["Feedback Begin"], "fullpath": "/nodes/top/feedbackbegin.html"}, " should have one work item per partial training."], "extent": [5637, 5740], "body": [{"type": "bullet_group", "body": [{"blevel": 6, "type": "bullet", "indent": 4, "text": ["For example, a ", {"scheme": "Node", "value": "/nodes/top/wedge", "type": "link", "text": ["Wedge node"], "fullpath": "/nodes/top/wedge.html"}, " can be put before ", {"scheme": "Node", "value": "/nodes/top/feedbackbegin", "type": "link", "text": ["Feedback Begin"], "fullpath": "/nodes/top/feedbackbegin.html"}, " on which ", {"type": "ui", "text": ["Wedge Count"]}, " specifies the number of partial training passes."], "extent": [5740, 5921]}], "container": true}], "container": true}, {"blevel": 2, "type": "bullet", "indent": 0, "text": ["The ", {"type": "ui", "text": ["Create Iterations From"]}, " on ", {"scheme": "Node", "value": "/nodes/top/feedbackbegin", "type": "link", "text": ["Feedback Begin"], "fullpath": "/nodes/top/feedbackbegin.html"}, " should be set to Upstream Items."], "extent": [5921, 6030]}, {"blevel": 2, "type": "bullet", "indent": 0, "text": ["The maximum number of epochs per partial training pass can be controlled by setting ", {"type": "ui", "text": ["Max Epochs"]}, " on ML Train Regression to some relatively small value such as 1024."], "extent": [6030, 6199]}, {"blevel": 2, "type": "bullet", "indent": 0, "text": ["To output and retain partially trained ONNX models, you can specify on the ML Train Regression node. "], "extent": [6199, 6303], "body": [{"type": "bullet_group", "body": [{"blevel": 6, "type": "bullet", "indent": 4, "text": ["In the Files tab, a ", {"type": "ui", "text": ["Model Base Name"]}, " that includes an attribute such as ", {"type": "code", "text": ["@wedgeindex"]}, "."], "extent": [6303, 6399]}], "container": true}], "container": true}, {"blevel": 2, "type": "bullet", "indent": 0, "text": ["To generate fresh training data before each partial training pass, you can put a ", {"scheme": "Node", "value": "/nodes/top/ropfetch", "type": "link", "text": ["ROP Fetch"], "fullpath": "/nodes/top/ropfetch.html"}, " that triggers the generation of a new data set in front of ML Train Regression inside the feedback loop."], "extent": [6399, 6618]}], "container": true}]}, {"level": 2, "id": null, "container": true, "type": "h", "indent": 0, "text": ["File Access"], "extent": [6618, 6636], "body": [{"type": "para", "indent": 0, "text": ["ML Regression Train accesses a variety of files. Each file name can be optionally modified by inserting one or more TOP attributes into a base name of a file. For example, ", {"type": "code", "text": ["@wedgeindex"]}, " or ", {"type": "code", "text": ["@loopiter"]}, "."], "extent": [6636, 6840]}, {"type": "para", "indent": 0, "text": ["This creates various types of TOP ML training setups such as:"], "extent": [6840, 6902]}, {"type": "bullet_group", "body": [{"blevel": 2, "type": "bullet", "indent": 0, "text": ["Training for varying hyperparameters with the use of a Wedge TOP,"], "extent": [6902, 6970]}, {"blevel": 2, "type": "bullet", "indent": 0, "text": ["Performing repeated partial training in a TOP feedback loop,"], "extent": [6970, 7033]}, {"blevel": 2, "type": "bullet", "indent": 0, "text": ["Generating additional training data on demand in a TOP feedback loop."], "extent": [7033, 7106]}], "container": true}, {"type": "para", "indent": 0, "text": ["Some of the files ML Regression Train accesses are read-only, both read from and written to,\nand only written to (or appended to)."], "extent": [7106, 7238]}, {"type": "para", "indent": 0, "text": ["In the read-only category, there is the data set, specified using ", {"type": "ui", "text": ["Data Set Folder"]}, " and ", {"type": "ui", "text": ["Data Set Base Name"]}, ". This provides the source for training data and validation data if early stopping is on."], "extent": [7238, 7441]}, {"type": "para", "indent": 0, "text": ["In the write-only category, there are partially trained models in ONNX format, with file names determined by ", {"type": "ui", "text": ["Models Folder"]}, " and ", {"type": "ui", "text": ["Model Base Name"]}, ". "], "extent": [7441, 7595]}, {"type": "para", "indent": 0, "text": ["The current state of the training is specified using ", {"type": "ui", "text": ["States Folder"]}, " and ", {"type": "ui", "text": ["State Base Name"]}, ".\nThe training state is read from at the start of a single invocation of ML RegressionTrain and written to at the end. The training state allows ML Regression Train to resume training where it left off in a previous invocation. This is valid when the state name is the same as the previous invocation."], "extent": [7595, 7992]}, {"type": "para", "indent": 0, "text": ["Diagnostic and progress information related to the training is logged to a file location controlled using ", {"type": "ui", "text": ["Logs Folder"]}, " and ", {"type": "ui", "text": ["Log Base Name"]}, ". If a log file exists, log information is appended to that existing log file. This logging allows the creation of a single log file if a single training is broken up into parts by invoking ML Regression Train multiple times. For example, in a TOP feedback loop."], "extent": [7992, 8399]}]}, {"level": 2, "id": null, "container": true, "type": "h", "indent": 0, "text": ["Limitations"], "extent": [8399, 8417], "body": [{"type": "para", "indent": 0, "text": ["ML Regression Train and its associated SOP nodes provide an easy starting point for experimentation with regression ML in Houdini. At some point you may outgrow this ML Regression Train node, because you need to add some training functionality that is specific to your problem. Examples may include other neural network architectures, a different regularization approach, a different cost function, and additional input data. In that case, you can extract and copy the internals of this node and its training script and use this as a basis for creating a modified training node. You may still be able to use many of the other tools on the SOP side of the example-based ML toolkit."], "extent": [8417, 9100]}]}, {"level": 1, "id": "parameters", "container": true, "type": "parameters_section", "indent": 0, "role": "section", "extent": [9100, 9112], "body": [{"level": 3, "type": "sep", "indent": 0, "text": [" Training "], "extent": [9112, 9130]}, {"level": 2, "type": "sep", "indent": 0, "text": [" Network Connectivity "], "extent": [9130, 9158], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Architecture"], "extent": [9158, 9177], "body": [{"type": "para", "indent": 8, "text": ["Architecture of the network. For example, Fully Connected Layers (MLP)."], "extent": [9204, 9285]}], "container": true, "attrs": {"id": "architecture"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Hidden Layer Format"], "extent": [9285, 9310], "body": [{"type": "para", "indent": 8, "text": ["The way the hidden layers are configured. For example, Uniform gives each hidden layer the same width and the same activation function."], "extent": [9342, 9487]}], "container": true, "attrs": {"id": "hiddenlayerformat"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Uniform Hidden Layers"], "extent": [9487, 9514], "body": [{"type": "para", "indent": 8, "text": ["Specify the amount of hidden layers."], "extent": [9548, 9594]}], "container": true, "attrs": {"id": "uniformhiddenlayers"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Uniform Hidden Width"], "extent": [9594, 9620], "body": [{"type": "para", "indent": 8, "text": ["Specify a common width for all hidden layers."], "extent": [9653, 9708]}], "container": true, "attrs": {"id": "uniformhiddenwidth"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Uniform Hidden Activation"], "extent": [9708, 9739], "body": [{"type": "para", "indent": 8, "text": ["The common type of activation function that is used for all hidden layers. For example, tanh (hyperbolic tangent)."], "extent": [9777, 9901]}], "container": true, "attrs": {"id": "uniformhiddenactivation"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 2, "type": "sep", "indent": 0, "text": [" Data Preparation "], "extent": [9901, 9924], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Shuffle"], "extent": [9924, 9938], "body": [{"type": "para", "indent": 8, "text": ["When on (recommended), the elements of the data set are re-ordered in a random order before the data set is used at all. Having this on ensures the validation set, which consists of the last contiguous part of the data set, will consist of random samples of the data set."], "extent": [9960, 10241]}], "container": true, "attrs": {"id": "shuffle"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Limit Size"], "extent": [10241, 10257], "body": [{"type": "para", "indent": 8, "text": ["When on, only an initial part of the data set is preserved. The remaining data is deleted. This step takes part right after shuffling, before any of the data is used for training. This option is useful for finding out how the generalization error of the trained model depends on the data size (making use of a Wedge TOP, for example). The resulting curve may inidicate whether more data would be beneficial to improve the generalization."], "extent": [10289, 10736]}], "container": true, "attrs": {"id": "limitsize"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Upper Limit"], "extent": [10736, 10753], "body": [{"type": "para", "indent": 8, "text": ["Specify an upper limit of the number of data sets that are preserved. The size of the remaining data set is the minimum of the initial data set size and this limit. "], "extent": [10778, 10962]}], "container": true, "attrs": {"id": "upperlimit"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 2, "type": "sep", "indent": 0, "text": [" Initialization "], "extent": [10962, 10983], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Random Seed"], "extent": [10983, 11001], "body": [{"type": "para", "indent": 8, "text": ["The random seed used to initialize the parameters of the neural network.\n        Different random seeds may result in different models, with different accuracies. This hyperparameter is a candidate for wedging."], "extent": [11035, 11255]}], "container": true, "attrs": {"id": "parameterrandomseed"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 2, "type": "sep", "indent": 0, "text": [" Training Granularity "], "extent": [11255, 11282], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Epochs per Evaluation"], "extent": [11282, 11310], "body": [{"type": "para", "indent": 8, "text": ["The number of epochs that are trained before each validation loss evaluation."], "extent": [11344, 11431]}], "container": true, "attrs": {"id": "epochsperevaluation"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Patience"], "extent": [11431, 11445], "body": [{"type": "para", "indent": 8, "text": ["The number of times the validation loss is evaluated without finding an improvement of the current best validation loss before giving up. "], "extent": [11468, 11616]}, {"type": "note_group", "body": [{"type": "note", "indent": 8, "role": "item", "extent": [11616, 11630], "body": [{"type": "para", "indent": 12, "text": ["This parameter is expressed in terms of the number of evaluations, not epochs. See the Epochs for Evaluation parameter to see how many epochs are trained between evaluations."], "extent": [11630, 11818]}], "container": true}], "container": true, "role": "item_group"}], "container": true, "attrs": {"id": "patience"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Max Batch Size"], "extent": [11818, 11838], "body": [{"type": "para", "indent": 8, "text": ["Upper limit on the number of labeled examples, randomly selected from the training set, that is considered for each optimization step.   "], "extent": [11865, 12012]}], "container": true, "attrs": {"id": "maxbatchsize"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 2, "type": "sep", "indent": 0, "text": [" Optimization "], "extent": [12012, 12031], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Algorithm"], "extent": [12031, 12047], "body": [{"type": "para", "indent": 9, "text": ["Choose the optimization algorithm that is used for training.\n         Currently, the only option is the Adam optimizer."], "extent": [12084, 12214]}], "container": true, "attrs": {"id": "optimizationalgorithm"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Learning Rate"], "extent": [12214, 12233], "body": [{"type": "para", "indent": 9, "text": ["This controls the step size that is used while training. The larger the step size, the larger each update of the network parameters tends to be. Choosing a smaller learning rate may take longer, but helps avoid locally optimal solutions being skipped over."], "extent": [12261, 12528]}], "container": true, "attrs": {"id": "learningrate"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Beta1"], "extent": [12528, 12539], "body": [{"type": "para", "indent": 9, "text": ["This coefficient is specific to the Adam optimization algorithm. Look online for more information about the Adam optimizer. See the PyTorch documentation, for example."], "extent": [12560, 12738]}], "container": true, "attrs": {"id": "beta1"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Beta2"], "extent": [12738, 12749], "body": [{"type": "para", "indent": 9, "text": ["This coefficient is specific to the Adam optimization algorithm. Look online for more information about the Adam optimizer. See the PyTorch documentation, for example."], "extent": [12770, 12948]}], "container": true, "attrs": {"id": "beta2"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 2, "type": "sep", "indent": 0, "text": [" Termination "], "extent": [12948, 12966], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Limit Epochs"], "extent": [12966, 12985], "body": [{"type": "para", "indent": 8, "text": ["Enforce a hard upper limit on the number of epochs during training."], "extent": [13011, 13088]}], "container": true, "attrs": {"id": "limitepochs"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Max Epochs"], "extent": [13088, 13104], "body": [{"type": "para", "indent": 8, "text": ["Hard upper limit on the number of epochs.\n        Setting this number too low may negatively affect the accuracy of the model."], "extent": [13128, 13264]}], "container": true, "attrs": {"id": "maxepochs"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 2, "type": "sep", "indent": 0, "text": [" Explicit Regularization "], "extent": [13264, 13294], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Weight Decay"], "extent": [13294, 13313], "body": [{"type": "para", "indent": 8, "text": ["The higher this value is set, the more the training session will try to keep the weights small (only the weights, not the biases). This is a very basic method for preventing overfitting."], "extent": [13339, 13535]}], "container": true, "attrs": {"id": "weightdecay"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 2, "type": "sep", "indent": 0, "text": [" Implicit Regularization "], "extent": [13535, 13565], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Enable Early Stopping"], "extent": [13565, 13593], "body": [{"type": "para", "indent": 8, "text": ["When on, stops the training as soon as the performance of the model on the validation set stops improving."], "extent": [13627, 13743]}], "container": true, "attrs": {"id": "enableearlystopping"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Training Data Proportion"], "extent": [13743, 13773], "body": [{"type": "para", "indent": 8, "text": ["The proportion of the data set that is used to train the model. This parameter applies only when ", {"type": "ui", "text": ["Enable Early Stopping"]}, " is enabled."], "extent": [13810, 13954]}], "container": true, "attrs": {"id": "trainingdataproportion"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Validation Data Proportion"], "extent": [13954, 13986], "body": [{"type": "para", "indent": 8, "text": ["The proportion of the data set that is used to validate the performance of the model. The validation set consists of a contiguous range of elements at the end of the data (after shuffling, if that\u2019s enabled). It is recommended to turn on the ", {"type": "ui", "text": ["Shuffle"]}, " option, otherwise the validation set will generally not consist of a random sample of the entire data set. This parameter applies only when ", {"type": "ui", "text": ["Enable Early Stopping"]}, " is enabled."], "extent": [14025, 14466]}], "container": true, "attrs": {"id": "validationdataproportion"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 3, "type": "sep", "indent": 0, "text": [" Files "], "extent": [14466, 14480], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Data Set Folder"], "extent": [14480, 14502], "body": [{"type": "para", "indent": 8, "text": ["Source folder that contains one or more data sets."], "extent": [14530, 14590]}], "container": true, "attrs": {"id": "datasetfolder"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Data Set Base Name"], "extent": [14590, 14614], "body": [{"type": "para", "indent": 8, "text": ["The base name of a data set, excluding the .raw extension.   "], "extent": [14644, 14715]}], "container": true, "attrs": {"id": "datasetbasename"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Models Folder"], "extent": [14715, 14734], "body": [{"type": "para", "indent": 8, "text": ["Destination folder for trained models."], "extent": [14761, 14809]}], "container": true, "attrs": {"id": "modelsfolder"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Models Base Name"], "extent": [14809, 14831], "body": [{"type": "para", "indent": 8, "text": ["The base name of a trained model, excluding the .onnx extension.   "], "extent": [14860, 14937]}], "container": true, "attrs": {"id": "modelsbasename"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["States Folder"], "extent": [14937, 14956], "body": [{"type": "para", "indent": 8, "text": ["Destination folder where the training node keeps the training state."], "extent": [14983, 15061]}], "container": true, "attrs": {"id": "statesfolder"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["States Base Name"], "extent": [15061, 15083], "body": [{"type": "para", "indent": 8, "text": ["The base name of a training state, excluding any extensions. This is the state the training node will be resumed."], "extent": [15112, 15235]}], "container": true, "attrs": {"id": "statesbasename"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Logs Folder"], "extent": [15235, 15252], "body": [{"type": "para", "indent": 8, "text": ["Folder that contains one or more training logs."], "extent": [15277, 15334]}], "container": true, "attrs": {"id": "logsfolder"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Logs Base Name"], "extent": [15334, 15354], "body": [{"type": "para", "indent": 8, "text": ["The base name of a training log, excluding the .txt extension.   "], "extent": [15381, 15456]}], "container": true, "attrs": {"id": "logsbasename"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 3, "type": "sep", "indent": 0, "text": [" Execution "], "extent": [15456, 15474]}, {"level": 2, "type": "sep", "indent": 0, "text": [" Device "], "extent": [15474, 15488], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 4, "text": ["Use CPU Exclusively"], "extent": [15488, 15514], "body": [{"type": "para", "indent": 8, "text": ["When on, the entire training is on the CPU, not using the GPU. This is not recommended as it is very slow. This option exists for debugging purposes."], "extent": [15546, 15705]}], "container": true, "attrs": {"id": "usecpuexclusively"}, "role": "item"}, {"type": "parameters_item", "indent": 4, "text": ["Environment Path"], "extent": [15705, 15727], "body": [{"type": "para", "indent": 8, "text": ["The path to the python virtual environment in which the internal training script of this node is run."], "extent": [15752, 15863]}], "container": true, "attrs": {"id": "venvpath"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}, {"level": 3, "type": "sep", "indent": 0, "text": [" Schedulers "], "extent": [15863, 15882], "body": [{"type": "parameters_item_group", "body": [{"type": "parameters_item", "indent": 0, "text": ["TOP Scheduler Override"], "extent": [6683, 6707], "body": [{"type": "para", "indent": 4, "text": ["This parameter overrides the TOP scheduler for this node."], "extent": [6730, 6793]}], "container": true, "attrs": {"id": "topscheduler"}, "role": "item"}, {"type": "parameters_item", "indent": 0, "text": ["Schedule When"], "extent": [10796, 10811], "body": [{"type": "para", "indent": 4, "text": ["When enabled, this parameter can be used to specify an expression that determines which work items from the node should be scheduled. If the expression returns zero for a given work item, that work item will immediately be marked as cooked instead of being queued with a scheduler. If the expression returns a non-zero value, the work item is scheduled normally."], "extent": [10838, 11206]}], "container": true, "attrs": {"id": "pdg_schedulewhen"}, "role": "item"}, {"type": "parameters_item", "indent": 0, "text": ["Work Item Label"], "extent": [9661, 9678], "body": [{"type": "para", "indent": 4, "text": ["Determines how the node should label its work items. This parameter allows you to assign non-unique label strings to your work items which are then used to identify the work items in the attribute panel, task bar, and scheduler job names."], "extent": [9706, 9950]}, {"type": "dt_group", "body": [{"type": "dt", "indent": 4, "text": ["Use Default Label"], "extent": [9950, 9974], "body": [{"type": "para", "indent": 8, "text": ["The work items in this node will use the default label from the TOP network, or have no label if the default is unset."], "extent": [9974, 10102]}], "container": true}, {"type": "dt", "indent": 4, "text": ["Inherit From Upstream Item"], "extent": [10102, 10135], "body": [{"type": "para", "indent": 8, "text": ["The work items inherit their labels from their parent work items.        "], "extent": [10135, 10218]}], "container": true}, {"type": "dt", "indent": 4, "text": ["Custom Expression"], "extent": [10218, 10242], "body": [{"type": "para", "indent": 8, "text": ["The work item label is set to the ", {"type": "ui", "text": ["Label Expression"]}, " custom expression which is evaluated for each item."], "extent": [10242, 10358]}], "container": true}, {"type": "dt", "indent": 4, "text": ["Node Defines Label"], "extent": [10358, 10383], "body": [{"type": "para", "indent": 8, "text": ["The work item label is defined in the node\u2019s internal logic."], "extent": [10383, 10453]}], "container": true}], "container": true}], "container": true, "attrs": {"id": "pdg_workitemlabel"}, "role": "item"}, {"type": "parameters_item", "indent": 0, "text": ["Label Expression"], "extent": [10453, 10471], "body": [{"type": "para", "indent": 4, "text": ["When on, this parameter specifies a custom label for work items created by this node. The parameter can be an expression that includes references to work item attributes or built-in properties. For example, ", {"type": "code", "text": ["$OS: @pdg_frame"]}, " will set the label of each work item based on its frame value."], "extent": [10503, 10796]}], "container": true, "attrs": {"id": "pdg_workitemlabelexpr"}, "role": "item"}, {"type": "parameters_item", "indent": 0, "text": ["Work Item Priority"], "extent": [7682, 7702], "body": [{"type": "para", "indent": 4, "text": ["This parameter determines how the current scheduler prioritizes the work items in this node."], "extent": [7733, 7831]}, {"type": "dt_group", "body": [{"type": "dt", "indent": 4, "text": ["Inherit From Upstream Item"], "extent": [7831, 7864], "body": [{"type": "para", "indent": 11, "text": ["The work items inherit their priority from their parent items. If a work item has no parent, its priority is set to 0."], "extent": [7864, 7995]}], "container": true}, {"type": "dt", "indent": 4, "text": ["Custom Expression"], "extent": [7995, 8019], "body": [{"type": "para", "indent": 11, "text": ["The work item priority is set to the value of ", {"type": "ui", "text": ["Priority Expression"]}, "."], "extent": [8019, 8102]}], "container": true}, {"type": "dt", "indent": 4, "text": ["Node Defines Priority"], "extent": [8102, 8130], "body": [{"type": "para", "indent": 7, "text": ["The work item priority is set based on the node\u2019s own internal priority calculations."], "extent": [8130, 8235]}, {"type": "para", "indent": 7, "text": ["This option is only available on the ", {"scheme": "Icon", "value": "MISC/python.svg", "type": "link", "text": "", "fullpath": "/nodes/top/MISC/python.svg.html"}, " ", {"scheme": "Node", "value": "/nodes/top/pythonprocessor", "type": "link", "text": ["Python Processor TOP"], "fullpath": "/nodes/top/pythonprocessor.html"}, ", ", {"scheme": "Icon", "value": "TOP/ropfetch.svg", "type": "link", "text": "", "fullpath": "/nodes/top/TOP/ropfetch.svg.html"}, " ", {"scheme": "Node", "value": "/nodes/top/ropfetch", "type": "link", "text": ["ROP Fetch TOP"], "fullpath": "/nodes/top/ropfetch.html"}, ", and ROP Output TOP nodes. These nodes define their own prioritization schemes that are implemented in their node logic."], "extent": [8235, 8531]}], "container": true}], "container": true}], "container": true, "attrs": {"id": "pdg_workitempriority"}, "role": "item"}, {"type": "parameters_item", "indent": 0, "text": ["Priority Expression"], "extent": [8531, 8552], "body": [{"type": "para", "indent": 4, "text": ["This parameter specifies an expression for work item priority. The expression is evaluated for each work item in the node."], "extent": [8587, 8715]}, {"type": "para", "indent": 4, "text": ["This parameter is only available when ", {"type": "ui", "text": ["Work Item Priority"]}, " is set to ", {"type": "ui", "text": ["Custom Expression"]}, "."], "extent": [8715, 8814]}], "container": true, "attrs": {"id": "pdg_workitempriorityexpr"}, "role": "item"}], "container": true, "role": "item_group"}], "container": true}], "text": "Parameters"}, {"level": 1, "id": "related", "container": true, "type": "related_section", "indent": 0, "role": "section", "extent": [16192, 16201], "body": [{"type": "bullet_group", "body": [{"blevel": 2, "type": "bullet", "indent": 0, "text": [{"scheme": null, "value": "/model/index", "type": "link", "text": ["Houdini geometry concepts"], "fullpath": "/model/index.html"}], "extent": [16201, 16246]}], "container": true}], "text": "Related"}], "title": ["ML Regression Train"], "summary": ["Trains a neural network for regression."], "included": ["/nodes/top/processor_common"]}